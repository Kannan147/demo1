{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPYFU+dQFzvfW+dHXktvxLk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kannan147/demo1/blob/main/Untitled24.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-gAnrTq6_CBH",
        "outputId": "4d620343-7342-41b1-d98d-531b10452b18"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word Alignment (English → French):\n",
            "the → la\n",
            "the → maison\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "\n",
        "# Sample parallel corpus (English → French)\n",
        "corpus = [\n",
        "    ([\"the\", \"house\"], [\"la\", \"maison\"]),\n",
        "    ([\"the\", \"book\"], [\"le\", \"livre\"])\n",
        "]\n",
        "\n",
        "# Initialize translation probabilities (Uniform distribution)\n",
        "translation_probs = defaultdict(lambda: defaultdict(lambda: 1.0))\n",
        "# Initialize transition probabilities (Assume uniform jumps initially)\n",
        "transition_probs = defaultdict(lambda: defaultdict(lambda: 1.0))\n",
        "\n",
        "def initialize_probabilities():\n",
        "    \"\"\" Initialize translation and transition probabilities uniformly \"\"\"\n",
        "    for eng_sent, fr_sent in corpus:\n",
        "        eng_vocab = set(eng_sent)\n",
        "        fr_vocab = set(fr_sent)\n",
        "\n",
        "        # Uniform translation probabilities\n",
        "        for e in eng_vocab:\n",
        "            for f in fr_vocab:\n",
        "                translation_probs[e][f] = 1.0 / len(fr_vocab)\n",
        "\n",
        "        # Uniform transition probabilities\n",
        "        for i in range(len(fr_sent)):\n",
        "            for j in range(len(eng_sent)):\n",
        "                transition_probs[i][j] = 1.0 / len(eng_sent)\n",
        "\n",
        "def expectation_step():\n",
        "    \"\"\" Compute expected counts using current probabilities \"\"\"\n",
        "    count_e_f = defaultdict(lambda: defaultdict(float))\n",
        "    total_f = defaultdict(float)\n",
        "    count_a = defaultdict(lambda: defaultdict(float))\n",
        "    total_a = defaultdict(float)\n",
        "\n",
        "    for eng_sent, fr_sent in corpus:\n",
        "        len_e = len(eng_sent)\n",
        "        len_f = len(fr_sent)\n",
        "\n",
        "        # Compute alignment probabilities for each French word\n",
        "        alignment_probs = np.zeros((len_f, len_e))\n",
        "        for j, f in enumerate(fr_sent):\n",
        "            normalization_factor = 0\n",
        "            for i, e in enumerate(eng_sent):\n",
        "                alignment_probs[j][i] = transition_probs[j][i] * translation_probs[e][f]\n",
        "                normalization_factor += alignment_probs[j][i]\n",
        "\n",
        "            # Normalize alignment probabilities\n",
        "            for i, e in enumerate(eng_sent):\n",
        "                alignment_probs[j][i] /= normalization_factor\n",
        "                count_e_f[e][f] += alignment_probs[j][i]\n",
        "                total_f[f] += alignment_probs[j][i]\n",
        "                count_a[j][i] += alignment_probs[j][i]\n",
        "                total_a[j] += alignment_probs[j][i]\n",
        "\n",
        "    return count_e_f, total_f, count_a, total_a\n",
        "\n",
        "def maximization_step(count_e_f, total_f, count_a, total_a):\n",
        "    \"\"\" Update translation and transition probabilities \"\"\"\n",
        "    for e in translation_probs:\n",
        "        for f in translation_probs[e]:\n",
        "            translation_probs[e][f] = count_e_f[e][f] / total_f[f] if total_f[f] > 0 else 0\n",
        "\n",
        "    for j in transition_probs:\n",
        "        for i in transition_probs[j]:\n",
        "            transition_probs[j][i] = count_a[j][i] / total_a[j] if total_a[j] > 0 else 0\n",
        "\n",
        "def train_hmm_mt(iterations=5):\n",
        "    \"\"\" Run the EM algorithm for a given number of iterations \"\"\"\n",
        "    initialize_probabilities()\n",
        "    for _ in range(iterations):\n",
        "        count_e_f, total_f, count_a, total_a = expectation_step()\n",
        "        maximization_step(count_e_f, total_f, count_a, total_a)\n",
        "\n",
        "def viterbi_align(eng_sent, fr_sent):\n",
        "    \"\"\" Perform alignment using the Viterbi algorithm \"\"\"\n",
        "    len_e = len(eng_sent)\n",
        "    len_f = len(fr_sent)\n",
        "    best_alignment = []\n",
        "\n",
        "    for j, f in enumerate(fr_sent):\n",
        "        best_prob = 0\n",
        "        best_index = 0\n",
        "        for i, e in enumerate(eng_sent):\n",
        "            prob = transition_probs[j][i] * translation_probs[e][f]\n",
        "            if prob > best_prob:\n",
        "                best_prob = prob\n",
        "                best_index = i\n",
        "        best_alignment.append((best_index, j))  # (English index, French index)\n",
        "\n",
        "    return best_alignment\n",
        "\n",
        "# Train HMM-based MT model\n",
        "train_hmm_mt()\n",
        "\n",
        "# Test alignment using Viterbi decoding\n",
        "eng_test = [\"the\", \"house\"]\n",
        "fr_test = [\"la\", \"maison\"]\n",
        "alignment = viterbi_align(eng_test, fr_test)\n",
        "\n",
        "# Print results\n",
        "print(\"Word Alignment (English → French):\")\n",
        "for eng_index, fr_index in alignment:\n",
        "    print(f\"{eng_test[eng_index]} → {fr_test[fr_index]}\")\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "\n",
        "# Example parallel corpus (English -> French)\n",
        "corpus = [\n",
        "    ([\"the\", \"house\"], [\"la\", \"maison\"]),\n",
        "    ([\"the\", \"book\"], [\"le\", \"livre\"]),\n",
        "    ([\"a\", \"house\"], [\"une\", \"maison\"]),\n",
        "    ([\"a\", \"book\"], [\"un\", \"livre\"]),\n",
        "    ([\"green\", \"house\"], [\"casa\", \"verde\"]),\n",
        "    ([\"the\", \"house\"], [\"la\", \"casa\"])\n",
        "]\n",
        "\n",
        "# Step 1: Initialize translation probabilities uniformly\n",
        "def initialize_translation_probabilities(corpus):\n",
        "    translation_probs = defaultdict(lambda: defaultdict(lambda: 1.0))\n",
        "    for eng_sentence, fr_sentence in corpus:\n",
        "        for e in eng_sentence:\n",
        "            for f in fr_sentence:\n",
        "                translation_probs[e][f] = 1.0 / len(fr_sentence)  # Uniform probability\n",
        "    return translation_probs\n",
        "\n",
        "# Step 2 & 3: Expectation-Maximization Algorithm\n",
        "def train_ibm_model_1(corpus, num_iterations=10):\n",
        "    # Initialize translation probabilities\n",
        "    t = initialize_translation_probabilities(corpus)\n",
        "\n",
        "    for iteration in range(num_iterations):\n",
        "        count = defaultdict(lambda: defaultdict(float))  # Expected counts\n",
        "        total = defaultdict(float)  # Total counts for normalization\n",
        "\n",
        "        # E-Step: Compute expected counts\n",
        "        for eng_sentence, fr_sentence in corpus:\n",
        "            for f in fr_sentence:\n",
        "                # Compute denominator for normalization\n",
        "                total_s = sum(t[e][f] for e in eng_sentence)\n",
        "                for e in eng_sentence:\n",
        "                    count[e][f] += t[e][f] / total_s\n",
        "                    total[e] += t[e][f] / total_s\n",
        "\n",
        "\n",
        "        # M-Step: Normalize to get updated probabilities\n",
        "\n",
        "        for e in count:\n",
        "            for f in count[e]:\n",
        "                t[e][f] = count[e][f] / total[e]  # Update translation probability\n",
        "                # print(e,f,t[e][f])\n",
        "\n",
        "        # Display progress\n",
        "        print(f\"Iteration {iteration + 1} complete.\")\n",
        "\n",
        "    return t\n",
        "\n",
        "# Train the IBM Model 1\n",
        "translation_probs = train_ibm_model_1(corpus, num_iterations=10)\n",
        "\n",
        "# Print learned translation probabilities\n",
        "for e in translation_probs:\n",
        "    print(f\"\\nTranslations for '{e}':\")\n",
        "    for f, prob in translation_probs[e].items():\n",
        "        print(f\"  P({f} | {e}) = {prob:.4f}\")\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "\n",
        "# Example parallel corpus (English -> French)\n",
        "corpus = [\n",
        "    ([\"the\", \"house\"], [\"la\", \"maison\"]),\n",
        "    ([\"the\", \"book\"], [\"le\", \"livre\"]),\n",
        "    ([\"a\", \"house\"], [\"une\", \"maison\"]),\n",
        "    ([\"a\", \"book\"], [\"un\", \"livre\"]),\n",
        "    ([\"green\", \"house\"], [\"casa\", \"verde\"]),\n",
        "    ([\"the\", \"house\"], [\"la\", \"casa\"])\n",
        "]\n",
        "\n",
        "# Step 1: Initialize translation probabilities uniformly\n",
        "def initialize_translation_probabilities(corpus):\n",
        "    translation_probs = defaultdict(lambda: defaultdict(lambda: 1.0))\n",
        "    for eng_sentence, fr_sentence in corpus:\n",
        "        for e in eng_sentence:\n",
        "            for f in fr_sentence:\n",
        "                translation_probs[e][f] = 1.0 / len(fr_sentence)  # Uniform probability\n",
        "    return translation_probs\n",
        "\n",
        "# Step 2 & 3: Expectation-Maximization Algorithm\n",
        "def train_ibm_model_1(corpus, num_iterations=10):\n",
        "    # Initialize translation probabilities\n",
        "    t = initialize_translation_probabilities(corpus)\n",
        "\n",
        "    for iteration in range(num_iterations):\n",
        "        count = defaultdict(lambda: defaultdict(float))  # Expected counts\n",
        "        total = defaultdict(float)  # Total counts for normalization\n",
        "\n",
        "        # E-Step: Compute expected counts\n",
        "        for eng_sentence, fr_sentence in corpus:\n",
        "            for f in fr_sentence:\n",
        "                # Compute denominator for normalization\n",
        "                total_s = sum(t[e][f] for e in eng_sentence)\n",
        "                for e in eng_sentence:\n",
        "                    count[e][f] += t[e][f] / total_s\n",
        "                    total[e] += t[e][f] / total_s\n",
        "\n",
        "\n",
        "        # M-Step: Normalize to get updated probabilities\n",
        "\n",
        "        for e in count:\n",
        "            for f in count[e]:\n",
        "                t[e][f] = count[e][f] / total[e]  # Update translation probability\n",
        "                # print(e,f,t[e][f])\n",
        "\n",
        "        # Display progress\n",
        "        print(f\"Iteration {iteration + 1} complete.\")\n",
        "\n",
        "    return t\n",
        "\n",
        "# Train the IBM Model 1\n",
        "translation_probs = train_ibm_model_1(corpus, num_iterations=10)\n",
        "\n",
        "# Print learned translation probabilities\n",
        "for e in translation_probs:\n",
        "    print(f\"\\nTranslations for '{e}':\")\n",
        "    for f, prob in translation_probs[e].items():\n",
        "        print(f\"  P({f} | {e}) = {prob:.4f}\")\n",
        "#Share excel including BLEU\n",
        "#Share the 2 pieces of code\n",
        "#has context menu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tqwNXVF4_wkI",
        "outputId": "956c734f-ca4a-4299-c9ce-a8a3225f3f0f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1 complete.\n",
            "Iteration 2 complete.\n",
            "Iteration 3 complete.\n",
            "Iteration 4 complete.\n",
            "Iteration 5 complete.\n",
            "Iteration 6 complete.\n",
            "Iteration 7 complete.\n",
            "Iteration 8 complete.\n",
            "Iteration 9 complete.\n",
            "Iteration 10 complete.\n",
            "\n",
            "Translations for 'the':\n",
            "  P(la | the) = 0.7352\n",
            "  P(maison | the) = 0.0171\n",
            "  P(le | the) = 0.2004\n",
            "  P(livre | the) = 0.0010\n",
            "  P(casa | the) = 0.0464\n",
            "\n",
            "Translations for 'house':\n",
            "  P(la | house) = 0.0395\n",
            "  P(maison | house) = 0.5336\n",
            "  P(une | house) = 0.0024\n",
            "  P(casa | house) = 0.4244\n",
            "  P(verde | house) = 0.0001\n",
            "\n",
            "Translations for 'book':\n",
            "  P(le | book) = 0.1910\n",
            "  P(livre | book) = 0.7701\n",
            "  P(un | book) = 0.0389\n",
            "\n",
            "Translations for 'a':\n",
            "  P(une | a) = 0.4906\n",
            "  P(maison | a) = 0.0576\n",
            "  P(un | a) = 0.4451\n",
            "  P(livre | a) = 0.0067\n",
            "\n",
            "Translations for 'green':\n",
            "  P(casa | green) = 0.2954\n",
            "  P(verde | green) = 0.7046\n",
            "Iteration 1 complete.\n",
            "Iteration 2 complete.\n",
            "Iteration 3 complete.\n",
            "Iteration 4 complete.\n",
            "Iteration 5 complete.\n",
            "Iteration 6 complete.\n",
            "Iteration 7 complete.\n",
            "Iteration 8 complete.\n",
            "Iteration 9 complete.\n",
            "Iteration 10 complete.\n",
            "\n",
            "Translations for 'the':\n",
            "  P(la | the) = 0.7352\n",
            "  P(maison | the) = 0.0171\n",
            "  P(le | the) = 0.2004\n",
            "  P(livre | the) = 0.0010\n",
            "  P(casa | the) = 0.0464\n",
            "\n",
            "Translations for 'house':\n",
            "  P(la | house) = 0.0395\n",
            "  P(maison | house) = 0.5336\n",
            "  P(une | house) = 0.0024\n",
            "  P(casa | house) = 0.4244\n",
            "  P(verde | house) = 0.0001\n",
            "\n",
            "Translations for 'book':\n",
            "  P(le | book) = 0.1910\n",
            "  P(livre | book) = 0.7701\n",
            "  P(un | book) = 0.0389\n",
            "\n",
            "Translations for 'a':\n",
            "  P(une | a) = 0.4906\n",
            "  P(maison | a) = 0.0576\n",
            "  P(un | a) = 0.4451\n",
            "  P(livre | a) = 0.0067\n",
            "\n",
            "Translations for 'green':\n",
            "  P(casa | green) = 0.2954\n",
            "  P(verde | green) = 0.7046\n"
          ]
        }
      ]
    }
  ]
}